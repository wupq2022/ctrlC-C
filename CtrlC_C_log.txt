2025-07-02 16:56:27,171 - INFO - removed newlines: Vector search. As shown in Algorithm 1, the search process follows a best-first approach. It starts at a fixed starting vector s and maintains a candidate pool P containing the nearest vectors to the target vector (sorted by their distances). In each step, it explores one or more nearest unexplored vectors, depending on the beam width. The search terminates when all the vectors in P are explored. Buffered insert. Algorithm 2 shows the process of inserting a vector q into a graph-based index. It first searches q and records the explored vectors in the search path, which are q’s candidate neighbors. Then, it attempts to add bi-directional edges to q and its candidate neighbors. If an edge set exceeds the out-degree limit, it is pruned according to some pre-defined rules (e.g., triangle inequality [21]). DiskANN adopts buffered insert, in order to reduce disk I/O for neighbor updates by batching them. Specifically, inserts are first absorbed by an extra in-memory index. When searching, both the in-memory and on-disk indexes are traversed. When the in-memory index size reaches a threshold, a merge is triggered, which includes two steps. First, it inserts
2025-07-02 16:56:27,177 - INFO - Stripped text: Vector search. As shown in Algorithm 1, the search process follows a best-first approach. It starts at a fixed starting vector s and maintains a candidate pool P containing the nearest vectors to the target vector (sorted by their distances). In each step, it explores one or more nearest unexplored vectors, depending on the beam width. The search terminates when all the vectors in P are explored. Buffered insert. Algorithm 2 shows the process of inserting a vector q into a graph-based index. It first searches q and records the explored vectors in the search path, which are q’s candidate neighbors. Then, it attempts to add bi-directional edges to q and its candidate neighbors. If an edge set exceeds the out-degree limit, it is pruned according to some pre-defined rules (e.g., triangle inequality [21]). DiskANN adopts buffered insert, in order to reduce disk I/O for neighbor updates by batching them. Specifically, inserts are first absorbed by an extra in-memory index. When searching, both the in-memory and on-disk indexes are traversed. When the in-memory index size reaches a threshold, a merge is triggered, which includes two steps. First, it inserts
2025-07-02 16:59:44,475 - INFO - 智能处理换行: Vector search. As shown in Algorithm 1, the search process follows a best-first approach. It starts at a fixed starting vector s and maintains a candidate pool P containing the nearest vectors to the target vector (sorted by their distances). In each step, it explores one or more nearest unexplored vectors, depending on the beam width. The search terminates when all the vectors in P are explored. Buffered insert. Algorithm 2 shows the process of inserting a vector q into a graph-based index. It first searches q and records the explored vectors in the search path, which are q’s candidate neighbors. Then, it attempts to add bi-directional edges to q and its candidate neighbors. If an edge set exceeds the out-degree limit, it is pruned according to some pre-defined rules (e.g., triangle inequality [21]). DiskANN adopts buffered insert, in order to reduce disk I/O for neighbor updates by batching them. Specifically, inserts are first absorbed by an extra in-memory index. When searching, both the in-memory and on-disk indexes are traversed. When the in-memory index size reaches a threshold, a merge is triggered, which includes t
2025-07-02 16:59:44,499 - INFO - Stripped text: Vector search. As shown in Algorithm 1, the search process follows a best-first approach. It starts at a fixed starting vector s and maintains a candidate pool P containing the nearest vectors to the target vector (sorted by their distances). In each step, it explores one or more nearest unexplored vectors, depending on the beam width. The search terminates when all the vectors in P are explored. Buffered insert. Algorithm 2 shows the process of inserting a vector q into a graph-based index. It first searches q and records the explored vectors in the search path, which are q’s candidate neighbors. Then, it attempts to add bi-directional edges to q and its candidate neighbors. If an edge set exceeds the out-degree limit, it is pruned according to some pre-defined rules (e.g., triangle inequality [21]). DiskANN adopts buffered insert, in order to reduce disk I/O for neighbor updates by batching them. Specifically, inserts are first absorbed by an extra in-memory index. When searching, both the in-memory and on-disk indexes are traversed. When the in-memory index size reaches a threshold, a merge is triggered, which includes t
2025-07-02 17:00:59,197 - INFO - 智能处理换行: Operations. To our knowledge, DiskANN [20] is the only on-disk graph-based ANNS index supporting updates. Here, we introduce its search and buffered insert operations. Vector search. As shown in Algorithm 1, the search process follows a best-first approach. It starts at a fixed starting vector s and maintains a candidate pool P containing the nearest vectors to the target vector (sorted by their distances). In each step, it explores one or more nearest unexplored vectors, depending on the beam width. The search terminates when all the vectors in P are explored. Buffered insert. Algorithm 2 shows the process of inserting a vector q into a graph-based index. It first searches q and records the explored vectors in the search path, which are q’s candidate neighbors. Then, it attempts to add bi-directional edges to q and its candidate neighbors. If an edge set exceeds the out-degree limit, it is pruned according to some pre-defined rules (e.g., triangle inequality [21]). DiskANN adopts buffered insert, in order to reduce disk I/O for neighbor updates by batching them. Specifically, inserts are first absorbed by an extra in-memory index. When searching, both the in-memory and on-disk indexes are traversed. When the in-memory index size reaches a threshold, a merge is triggered, which includes two
2025-07-02 17:00:59,217 - INFO - Stripped text: Operations. To our knowledge, DiskANN [20] is the only on-disk graph-based ANNS index supporting updates. Here, we introduce its search and buffered insert operations. Vector search. As shown in Algorithm 1, the search process follows a best-first approach. It starts at a fixed starting vector s and maintains a candidate pool P containing the nearest vectors to the target vector (sorted by their distances). In each step, it explores one or more nearest unexplored vectors, depending on the beam width. The search terminates when all the vectors in P are explored. Buffered insert. Algorithm 2 shows the process of inserting a vector q into a graph-based index. It first searches q and records the explored vectors in the search path, which are q’s candidate neighbors. Then, it attempts to add bi-directional edges to q and its candidate neighbors. If an edge set exceeds the out-degree limit, it is pruned according to some pre-defined rules (e.g., triangle inequality [21]). DiskANN adopts buffered insert, in order to reduce disk I/O for neighbor updates by batching them. Specifically, inserts are first absorbed by an extra in-memory index. When searching, both the in-memory and on-disk indexes are traversed. When the in-memory index size reaches a threshold, a merge is triggered, which includes two
2025-07-02 17:01:16,863 - INFO - 智能处理换行: and do not insert vectors during merge. Merge will account for a higher time percentage in workloads with higher insert throughput or serving inserts during the merge. One may consider increasing the batch size to reduce the time percentage of merge. However, merge gains little performance boost even with a large batch. As shown in Figure 2(c), the throughput of merge increases with the batch size, but is bottlenecked by in-memory merge, whose throughput remains constant regardless of the batch size. This is because, although disk writes are merged, the in-memory merge is still executed one by one. For each vector, it searches the on-disk index for neighbors and incurs hundreds of disk reads. 2.3 Opportunity: Direct Insert We propose to do direct insert instead of buffered insert. Using direct insert, vectors are directly inserted into the on-disk index one by one, instead of buffered in memory and merged to disk later as in buffered insert. Direct insert can naturally solve the three issues above. First, it evens out the bandwidth interference across the whole timeline, which contributes to stable frontend search performance. Second, it avoids both the in-memory index and buffered disk writes during merge, thus saving memory. Third, it is possible for direct insert to be as efficient as buffered insert, as inserts can not be efficiently batched. However, it is non-trivial to do direct insert efficiently. There are several challenges as follows.
2025-07-02 17:01:16,868 - INFO - Stripped text: and do not insert vectors during merge. Merge will account for a higher time percentage in workloads with higher insert throughput or serving inserts during the merge. One may consider increasing the batch size to reduce the time percentage of merge. However, merge gains little performance boost even with a large batch. As shown in Figure 2(c), the throughput of merge increases with the batch size, but is bottlenecked by in-memory merge, whose throughput remains constant regardless of the batch size. This is because, although disk writes are merged, the in-memory merge is still executed one by one. For each vector, it searches the on-disk index for neighbors and incurs hundreds of disk reads. 2.3 Opportunity: Direct Insert We propose to do direct insert instead of buffered insert. Using direct insert, vectors are directly inserted into the on-disk index one by one, instead of buffered in memory and merged to disk later as in buffered insert. Direct insert can naturally solve the three issues above. First, it evens out the bandwidth interference across the whole timeline, which contributes to stable frontend search performance. Second, it avoids both the in-memory index and buffered disk writes during merge, thus saving memory. Third, it is possible for direct insert to be as efficient as buffered insert, as inserts can not be efficiently batched. However, it is non-trivial to do direct insert efficiently. There are several challenges as follows.
2025-07-02 17:01:33,172 - INFO - 智能处理换行: One may consider increasing the batch size to reduce the time percentage of merge. However, merge gains little performance boost even with a large batch. As shown in Figure 2(c), the throughput of merge increases with the batch size, but is bottlenecked by in-memory merge, whose throughput remains constant regardless of the batch size. This is because, although disk writes are merged, the in-memory merge is still executed one by one. For each vector, it searches the on-disk index for neighbors and incurs hundreds of disk reads. 2.3 Opportunity: Direct Insert We propose to do direct insert instead of buffered insert. Using direct insert, vectors are directly inserted into the on-disk index one by one, instead of buffered in memory and merged to disk later as in buffered insert. Direct insert can naturally solve the three issues above. First, it evens out the bandwidth interference across the whole timeline, which contributes to stable frontend search performance. Second, it avoids both the in-memory index and buffered disk writes during merge, thus saving memory. Third, it is possible for direct insert to be as efficient as buffered insert, as inserts can not be efficiently batched. However, it is non-trivial to do direct insert efficiently. There are several challenges as follows. Challenge 1. Each insert updates tens o
2025-07-02 17:01:33,192 - INFO - Stripped text: One may consider increasing the batch size to reduce the time percentage of merge. However, merge gains little performance boost even with a large batch. As shown in Figure 2(c), the throughput of merge increases with the batch size, but is bottlenecked by in-memory merge, whose throughput remains constant regardless of the batch size. This is because, although disk writes are merged, the in-memory merge is still executed one by one. For each vector, it searches the on-disk index for neighbors and incurs hundreds of disk reads. 2.3 Opportunity: Direct Insert We propose to do direct insert instead of buffered insert. Using direct insert, vectors are directly inserted into the on-disk index one by one, instead of buffered in memory and merged to disk later as in buffered insert. Direct insert can naturally solve the three issues above. First, it evens out the bandwidth interference across the whole timeline, which contributes to stable frontend search performance. Second, it avoids both the in-memory index and buffered disk writes during merge, thus saving memory. Third, it is possible for direct insert to be as efficient as buffered insert, as inserts can not be efficiently batched. However, it is non-trivial to do direct insert efficiently. There are several challenges as follows. Challenge 1. Each insert updates tens o
2025-07-02 17:05:36,753 - INFO - 智能处理换行: 通过以下规则识别段落换行：

1. 连续的两个或以上的换行符视为段落分隔 2. 以句号、问号、感叹号等结尾的行后的换行视为段落分隔，还需要支持英文的各种结尾符号 3. 其他情况的单个换行符视为PDF复制产生的行内换行，替换为空格
2025-07-02 17:05:36,769 - INFO - Stripped text: 通过以下规则识别段落换行：

1. 连续的两个或以上的换行符视为段落分隔 2. 以句号、问号、感叹号等结尾的行后的换行视为段落分隔，还需要支持英文的各种结尾符号 3. 其他情况的单个换行符视为PDF复制产生的行内换行，替换为空格
2025-07-02 17:06:19,312 - INFO - 智能处理换行: One may consider increasing the batch size to reduce the time percentage of merge. However, merge gains little performance boost even with a large batch. As shown in Figure 2(c), the throughput of merge increases with the batch size, but is bottlenecked by in-memory merge, whose throughput remains constant regardless of the batch size. This is because, although disk writes are merged, the in-memory merge is still executed one by one. For each vector, it searches the on-disk index for neighbors and incurs hundreds of disk reads. 2.3 Opportunity: Direct Insert We propose to do direct insert instead of buffered insert. Using direct insert, vectors are directly inserted into the on-disk index one by one, instead of buffered in memory and merged to disk later as in buffered insert. Direct insert can naturally solve the three issues above. First, it evens out the bandwidth interference across the whole timeline, which contributes to stable frontend search performance. Second, it avoids both the in-memory index and buffered disk writes during merge, thus saving memory. Third, it is possible for direct insert to be as efficient as buffered insert, as inserts can not be efficient
2025-07-02 17:06:19,329 - INFO - Stripped text: One may consider increasing the batch size to reduce the time percentage of merge. However, merge gains little performance boost even with a large batch. As shown in Figure 2(c), the throughput of merge increases with the batch size, but is bottlenecked by in-memory merge, whose throughput remains constant regardless of the batch size. This is because, although disk writes are merged, the in-memory merge is still executed one by one. For each vector, it searches the on-disk index for neighbors and incurs hundreds of disk reads. 2.3 Opportunity: Direct Insert We propose to do direct insert instead of buffered insert. Using direct insert, vectors are directly inserted into the on-disk index one by one, instead of buffered in memory and merged to disk later as in buffered insert. Direct insert can naturally solve the three issues above. First, it evens out the bandwidth interference across the whole timeline, which contributes to stable frontend search performance. Second, it avoids both the in-memory index and buffered disk writes during merge, thus saving memory. Third, it is possible for direct insert to be as efficient as buffered insert, as inserts can not be efficient
2025-07-02 17:09:19,838 - INFO - 智能处理换行: r a higher time percentage in workloads with higher insert throughput or serving inserts during the merge.

One may consider increasing the batch size to reduce the time percentage of merge. However, merge gains little performance boost even with a large batch. As shown in Figure 2(c), the throughput of merge increases with the batch size, but is bottlenecked by in-memory merge, whose throughput remains constant regardless of the batch size. This is because, although disk writes are merged, the in-memory merge is still executed one by one. For each vector, it searches the on-disk index for neighbors and incurs hundreds of disk reads.

2.3 Opportunity: Direct Insert We propose to do direct insert instead of buffered insert. Using direct insert, vectors are directly inserted into the on-disk index one by one, instead of buffered in memory and merged to disk later as in buffered insert.

Direct insert can naturally solve the three issues above.

First, it evens out the bandwidth interference across the whole timeline, which contributes to stable frontend search performance. Second, it avoids both the in-memory index and buffered disk writes during merge, thus saving memory. Third, it is possible for direct insert to be as efficient as buffered insert, as inserts can not be efficiently batched.

However, it is non-trivial to do direct insert efficiently.

There are several challenges as follows.
2025-07-02 17:09:19,845 - INFO - Stripped text: r a higher time percentage in workloads with higher insert throughput or serving inserts during the merge.

One may consider increasing the batch size to reduce the time percentage of merge. However, merge gains little performance boost even with a large batch. As shown in Figure 2(c), the throughput of merge increases with the batch size, but is bottlenecked by in-memory merge, whose throughput remains constant regardless of the batch size. This is because, although disk writes are merged, the in-memory merge is still executed one by one. For each vector, it searches the on-disk index for neighbors and incurs hundreds of disk reads.

2.3 Opportunity: Direct Insert We propose to do direct insert instead of buffered insert. Using direct insert, vectors are directly inserted into the on-disk index one by one, instead of buffered in memory and merged to disk later as in buffered insert.

Direct insert can naturally solve the three issues above.

First, it evens out the bandwidth interference across the whole timeline, which contributes to stable frontend search performance. Second, it avoids both the in-memory index and buffered disk writes during merge, thus saving memory. Third, it is possible for direct insert to be as efficient as buffered insert, as inserts can not be efficiently batched.

However, it is non-trivial to do direct insert efficiently.

There are several challenges as follows.
2025-07-02 17:09:51,699 - INFO - 智能处理换行: 
2025-07-02 17:09:51,714 - INFO - Stripped text: 
2025-07-02 17:10:04,759 - INFO - 智能处理换行: 1 Introduction Approximate nearest neighbor search (ANNS) is the key to multi-modal data retrieval in web search [8, 15] and retrievalaugmented generation (RAG) [13]. Multi-modal data, such as texts and images, are encoded to high-dimensional vectors using neural networks [7, 17]. An ANNS index is built upon these vectors and traversed during a search to get the k nearest neighbors of the query vector. Among all types of ANNS indexes, on-disk graph-based indexes [21,24], in which vectors are organized as a graph and stored on disk, are favored for their performance and cost-efficiency to support large-scale vector search (e.g., billions of vectors [5, 19, 26]).

There is a strong need for ANNS indexes to support vector updates because current systems generate new data continuously [14, 23, 26, 27]. Vector updates can be handled in two ways, index rebuild and index update. Index rebuild, which takes several days in billion-scale datasets [27], fails to keep search results up-to-date. Therefore, index update is favored by recent on-disk graph-based indexes [20]. They support vector inserts and deletes using buffered insert and delete, where they absorb updates into an in-memory index, and then bulk merge the in-memory index into the on-disk index periodically. This way reduces the overhead of index updates by combining disk writes during merge, thus enabling real-time vector search and online index update.

However, by experimental analysis, we find that buffered insert is inefficient when merging inserts to disk. First, merge interferes with frontend search tasks (e.g., increases their median latency to 1.54×), owing to disk bandwidth interference.

Second, merge has a high memory consumption (e.g., 125GB for merging 3% of vectors into a billion-scale index [20]), which consists of the in-memory index for inserts and buffered disk writes during the merge. Third, merge gains little performance boost even with a large insert batch, whose throughput is capped at 3000 QPS in our experiment. This is because merge first finds the neighbors for the inserted vectors using on-disk ANNS. This process can not be batched efficiently and thus is executed one by one.

In this paper, we propose to use direct insert for on-disk graph-based ANNS indexes, where vectors are directly inserted into the on-disk index one by one, rather than buffered in memory and merged to disk later as in buffered insert. This approach can even out the insert cost and avoid the in-memory index used by buffered insert, thus stabilizing the frontend search performance and saving memory. Also, it is possible to make direct insert as fast as buffered insert, because buffered insert does not benefit much from batching. However, it is non-trivial to achieve efficient direct insert in graph-based indexes. There are several challenges as follows.

(1) High disk write overhead of direct insert. Direct insert updates tens or hundreds of neighbor records of the target vector. Updating them in place incurs massive disk writes. To combine these record updates for fewer disk writes, a straightforward way is to use a log-structured data layout, but it suffers from garbage collection (GC) overhead.

(2) Complex concurrency control with search. Direct insert searches the target vector to find its neighbors, which are scattered along the search path. For concurrency control, simply locking all of them before updating causes near-root lock contention. An alternative approach is to leverage the approximate features of insert and search for better concurrency, but how to do this efficiently on disk is still under-exploited.

To address the challenges, we propose OdinANN, a billionscale graph-based ANNS index with direct inserts. OdinANN achieves efficient direct insert with two techniques as follows.

First, to reduce disk write overhead, we propose GC-free update combining. The key observation is that the graph index is stored as fixed-size records on disk. This enables out-ofplace record updates to be performed without garbage collection. Therefore, we do space overprovision on disk to reserve multiple free record slots in a page, where multiple record updates can be combined. The old records can be directly re-
2025-07-02 17:10:04,766 - INFO - Stripped text: 1 Introduction Approximate nearest neighbor search (ANNS) is the key to multi-modal data retrieval in web search [8, 15] and retrievalaugmented generation (RAG) [13]. Multi-modal data, such as texts and images, are encoded to high-dimensional vectors using neural networks [7, 17]. An ANNS index is built upon these vectors and traversed during a search to get the k nearest neighbors of the query vector. Among all types of ANNS indexes, on-disk graph-based indexes [21,24], in which vectors are organized as a graph and stored on disk, are favored for their performance and cost-efficiency to support large-scale vector search (e.g., billions of vectors [5, 19, 26]).

There is a strong need for ANNS indexes to support vector updates because current systems generate new data continuously [14, 23, 26, 27]. Vector updates can be handled in two ways, index rebuild and index update. Index rebuild, which takes several days in billion-scale datasets [27], fails to keep search results up-to-date. Therefore, index update is favored by recent on-disk graph-based indexes [20]. They support vector inserts and deletes using buffered insert and delete, where they absorb updates into an in-memory index, and then bulk merge the in-memory index into the on-disk index periodically. This way reduces the overhead of index updates by combining disk writes during merge, thus enabling real-time vector search and online index update.

However, by experimental analysis, we find that buffered insert is inefficient when merging inserts to disk. First, merge interferes with frontend search tasks (e.g., increases their median latency to 1.54×), owing to disk bandwidth interference.

Second, merge has a high memory consumption (e.g., 125GB for merging 3% of vectors into a billion-scale index [20]), which consists of the in-memory index for inserts and buffered disk writes during the merge. Third, merge gains little performance boost even with a large insert batch, whose throughput is capped at 3000 QPS in our experiment. This is because merge first finds the neighbors for the inserted vectors using on-disk ANNS. This process can not be batched efficiently and thus is executed one by one.

In this paper, we propose to use direct insert for on-disk graph-based ANNS indexes, where vectors are directly inserted into the on-disk index one by one, rather than buffered in memory and merged to disk later as in buffered insert. This approach can even out the insert cost and avoid the in-memory index used by buffered insert, thus stabilizing the frontend search performance and saving memory. Also, it is possible to make direct insert as fast as buffered insert, because buffered insert does not benefit much from batching. However, it is non-trivial to achieve efficient direct insert in graph-based indexes. There are several challenges as follows.

(1) High disk write overhead of direct insert. Direct insert updates tens or hundreds of neighbor records of the target vector. Updating them in place incurs massive disk writes. To combine these record updates for fewer disk writes, a straightforward way is to use a log-structured data layout, but it suffers from garbage collection (GC) overhead.

(2) Complex concurrency control with search. Direct insert searches the target vector to find its neighbors, which are scattered along the search path. For concurrency control, simply locking all of them before updating causes near-root lock contention. An alternative approach is to leverage the approximate features of insert and search for better concurrency, but how to do this efficiently on disk is still under-exploited.

To address the challenges, we propose OdinANN, a billionscale graph-based ANNS index with direct inserts. OdinANN achieves efficient direct insert with two techniques as follows.

First, to reduce disk write overhead, we propose GC-free update combining. The key observation is that the graph index is stored as fixed-size records on disk. This enables out-ofplace record updates to be performed without garbage collection. Therefore, we do space overprovision on disk to reserve multiple free record slots in a page, where multiple record updates can be combined. The old records can be directly re-
2025-07-02 17:10:15,634 - INFO - 智能处理换行: cycled without GC. By default, OdinANN doubles disk space to combine updates, resulting in 2× disk writes compared to a log-structured layout.

Second, to increase the concurrency of insert and search, we introduce approximate concurrency control, leveraging the approximate features of operations for better parallelism.

The key idea is to ensure per-record isolations rather than peroperation ones. For vector search, only a consistent snapshot for each record is required to ensure its correctness. For vector insert, it links the target vector with an approximate neighbor snapshot. Besides, we propose two optimizations for insert, tailored for disk I/O and computing in the critical sections separately, to further improve the concurrency.

We compare OdinANN with state-of-the-art ANNS indexes supporting updates, including graph-based DiskANN [20] and cluster-based SPFresh [27]. In workloads with concurrent inserts and searches, OdinANN shows stable performance, with a median search latency fluctuation1 of only 1.07×, compared to DiskANN’s 2.44×. Compared to SPFresh, OdinANN achieves 62.1% median search latency and \u223c15% higher accuracy simultaneously, because of the intrinsic advantage of graph-based indexes over cluster-based ones. In billion-scale datasets, OdinANN simultaneously reaches 5000 QPS search throughput and 1100 QPS insert throughput, with a consistently stable median search latency of \u223c3ms.

In summary, this paper makes the following contributions:

\u2022 We analyze the inefficiency of buffered inserts for on-disk graph-based ANNS indexes (§2).

\u2022 We propose OdinANN, a billion-scale graph-based ANNS index with direct inserts. It achieves efficient direct inserts by two key techniques, GC-free update combining, and approximate concurrency control (§3).

\u2022 We evaluate OdinANN to show its efficacy in stabilizing the search performance and reducing memory usage during concurrent inserts and searches (§4).
2025-07-02 17:10:15,654 - INFO - Stripped text: cycled without GC. By default, OdinANN doubles disk space to combine updates, resulting in 2× disk writes compared to a log-structured layout.

Second, to increase the concurrency of insert and search, we introduce approximate concurrency control, leveraging the approximate features of operations for better parallelism.

The key idea is to ensure per-record isolations rather than peroperation ones. For vector search, only a consistent snapshot for each record is required to ensure its correctness. For vector insert, it links the target vector with an approximate neighbor snapshot. Besides, we propose two optimizations for insert, tailored for disk I/O and computing in the critical sections separately, to further improve the concurrency.

We compare OdinANN with state-of-the-art ANNS indexes supporting updates, including graph-based DiskANN [20] and cluster-based SPFresh [27]. In workloads with concurrent inserts and searches, OdinANN shows stable performance, with a median search latency fluctuation1 of only 1.07×, compared to DiskANN’s 2.44×. Compared to SPFresh, OdinANN achieves 62.1% median search latency and \u223c15% higher accuracy simultaneously, because of the intrinsic advantage of graph-based indexes over cluster-based ones. In billion-scale datasets, OdinANN simultaneously reaches 5000 QPS search throughput and 1100 QPS insert throughput, with a consistently stable median search latency of \u223c3ms.

In summary, this paper makes the following contributions:

\u2022 We analyze the inefficiency of buffered inserts for on-disk graph-based ANNS indexes (§2).

\u2022 We propose OdinANN, a billion-scale graph-based ANNS index with direct inserts. It achieves efficient direct inserts by two key techniques, GC-free update combining, and approximate concurrency control (§3).

\u2022 We evaluate OdinANN to show its efficacy in stabilizing the search performance and reducing memory usage during concurrent inserts and searches (§4).
2025-07-02 17:11:10,892 - INFO - 智能处理换行: cycled without GC. By default, OdinANN doubles disk space to combine updates, resulting in 2× disk writes compared to a log-structured layout.

Second, to increase the concurrency of insert and search, we introduce approximate concurrency control, leveraging the approximate features of operations for better parallelism.

The key idea is to ensure per-record isolations rather than peroperation ones. For vector search, only a consistent snapshot for each record is required to ensure its correctness. For vector insert, it links the target vector with an approximate neighbor snapshot. Besides, we propose two optimizations for insert, tailored for disk I/O and computing in the critical sections separately, to further improve the concurrency.

We compare OdinANN with state-of-the-art ANNS indexes supporting updates, including graph-based DiskANN [20] and cluster-based SPFresh [27]. In workloads with concurrent inserts and searches, OdinANN shows stable performance, with a median search latency fluctuation1 of only 1.07×, compared to DiskANN’s 2.44×. Compared to SPFresh, OdinANN achieves 62.1% median search latency and \u223c15% higher accuracy simultaneously, because of the intrinsic advantage of graph-based indexes over cluster-based ones. In billion-scale datasets, OdinANN simultaneously reaches 5000 QPS search throughput and 1100 QPS insert throughput, with a consistently stable median search latency of \u223c3ms.

In summary, this paper makes the following contributions:

\u2022 We analyze the inefficiency of buffered inserts for on-disk graph-based ANNS indexes (§2).

\u2022 We propose OdinANN, a billion-scale graph-based ANNS index with direct inserts. It achieves efficient direct inserts by two key techniques, GC-free update combining, and approximate concurrency control (§3).

\u2022 We evaluate OdinANN to show its efficacy in stabilizing the search performance and reducing memory usage during concurrent inserts and searches (§4).
2025-07-02 17:11:10,907 - INFO - Stripped text: cycled without GC. By default, OdinANN doubles disk space to combine updates, resulting in 2× disk writes compared to a log-structured layout.

Second, to increase the concurrency of insert and search, we introduce approximate concurrency control, leveraging the approximate features of operations for better parallelism.

The key idea is to ensure per-record isolations rather than peroperation ones. For vector search, only a consistent snapshot for each record is required to ensure its correctness. For vector insert, it links the target vector with an approximate neighbor snapshot. Besides, we propose two optimizations for insert, tailored for disk I/O and computing in the critical sections separately, to further improve the concurrency.

We compare OdinANN with state-of-the-art ANNS indexes supporting updates, including graph-based DiskANN [20] and cluster-based SPFresh [27]. In workloads with concurrent inserts and searches, OdinANN shows stable performance, with a median search latency fluctuation1 of only 1.07×, compared to DiskANN’s 2.44×. Compared to SPFresh, OdinANN achieves 62.1% median search latency and \u223c15% higher accuracy simultaneously, because of the intrinsic advantage of graph-based indexes over cluster-based ones. In billion-scale datasets, OdinANN simultaneously reaches 5000 QPS search throughput and 1100 QPS insert throughput, with a consistently stable median search latency of \u223c3ms.

In summary, this paper makes the following contributions:

\u2022 We analyze the inefficiency of buffered inserts for on-disk graph-based ANNS indexes (§2).

\u2022 We propose OdinANN, a billion-scale graph-based ANNS index with direct inserts. It achieves efficient direct inserts by two key techniques, GC-free update combining, and approximate concurrency control (§3).

\u2022 We evaluate OdinANN to show its efficacy in stabilizing the search performance and reducing memory usage during concurrent inserts and searches (§4).
